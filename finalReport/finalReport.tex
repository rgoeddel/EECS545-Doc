\documentclass[11pt]{article}

\usepackage[margin=1.0in]{geometry}
\usepackage{fullpage}
\usepackage{url}
\usepackage{paralist}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage[small]{caption}
\usepackage{subfig}
\usepackage{multirow}
\usepackage{color}


%%% TO DO
%-- Methodology
%    -- more thoroughly explain nudging selection process (James)
%-- Evaluation
%    -- paragraph explaining scenario


% The xxx tag is intended to denote urgent text that needs addressing.
% The meh tag is intended to highlight text that needs some loving or which
% we're not sure should make primetime
\newcommand{\xxx}[1]{{\bf \color{red} #1}}
\newcommand{\meh}[1]{{\bf \color{blue} #1}}
\newcommand\T{\rule{0pt}{3ex}}
\newcommand\B{\rule[-1.2ex]{0pt}{0pt}}

\title{Reducing Uncertainty and Acquiring Knowledge Through Interaction}
\author{Rob Goeddel \and Lauren Hinkle \and James Kirk \and Aaron Mininger}
\date{}

\begin{document}
\maketitle


\begin{abstract}
When an agent is faced with uncertainty it can take two main courses of action: it
can explore on its own to gain more knowledge or it can consult an expert. We
have created an object recognition system that utilizes both sources of
knowledge to improve its performance. Our system captures point cloud data
from an RGB-D camera and uses machine learning techniques to extract features
and classify properties of objects including size, shape, and color. It is
able to measure the amount of uncertainty for a given property and manipulate
the object using a robotic arm in order to reduce that uncertainty. It also
incorporates direct feedback from the user to further improve its
classification performance and requests more information when it misclassifies
an object. Users can teach the system new property labels on the fly.
We demonstrate our system through a simple I-Spy game with toy blocks of
various shapes and colors.
\end{abstract}


\section{Introduction}
%\xxx{Problem description and motivation. Why do you want to solve this
%    problem? What's the impact if you can solve this problem?}
Agents that operate in environments that are partially observable or very
noisy are often faced with large amounts of uncertainty. One option for the
agent is to explore the environment to resolve some of that uncertainty. While
this approach can be done autonomously, there
still may be fundamental limits to what information the agent is able to
collect on its own. A second option is for the agent to consult an expert to acquire
missing knowledge, a technique known as active learning. This can present a
large and rich source of knowledge, but can be costly to acquire. Hybrid
approaches enable an agent to explore the world on its own and ask an expert
for knowledge only when necessary. This hybridization leverages the
strengths of each option and can lead to better performance. Of course,
          integrating them introduces new challenges such as how to tell when
          knowledge is unattainable through exploration or what constitutes an
          important question.

Our project to utilizes both sources of knowledge.
Our system is tasked with identifying properties of various objects like
color, shape, and size in order to play the children's game ``I spy''.
Unfortunately, objects are commonly positioned in poor orientations with
respect to our vision system and are
thus difficult for our system to classify. We use both recent history and
information from a K-Nearest Neighbor (KNN) classifier to determine when we
are uncertain about an object. Instead of asking the user for help, the system
first tries to gain more information about the object by nudging it using a
robotic arm. The goal is to change the orientation of the object in order to
improve its classification performance. It will learn which classes are
inherently uncertain (such as rectangles) and adjust accordingly. The system also
ask for feedback from the user when an object is misidentified to gain more
training data. Our system is flexible enough to allow users to teach new
classes of properties labels during its operation. This work is being incorporated
into a larger project which includes teaching the system new concepts and actions.
The project relies on having the symbolic information provided by our work through
object segmentation and classification. More information on this project is
discussed in Sec.~\ref{sec:future-work}.

The symbolic information provided by our system can be analyzed and combined in
order to build new concepts such as object categories (like blocks and balls)
or object properties (bananas are yellow). Such reasoning and concept generalization
is essential for agents that interact in novel situations.

The challenges involved in playing I Spy include object detection, feature
extraction, classification, robot arm manipulation, and active learning. We
discuss these further in Section 3.

\section{Related Work}
%\xxx{What are existing methods? What are the state-of-the-art methods for this
%    problem? How is your approach different from the related work?}

Our work spans a number of domains, with aspects similar to projects in image
recognition, robotics, instruction, and active learning. There is a rich
history of work in object recognition within computer vision, and some more
recent efforts have focused on generalizing objects into classes like cars,
bikes, or trucks~\cite{huber2004parts}. Other work has focused on
constructing classifiers that build intermediate attribute representations,
such as the number of petals on a flower, to provide more robust
identification~\cite{nilsback2006visual,gehler2009feature}.
Sensors that combine both image and depth information, like the Microsoft Kinect,
provide richer sensory information to aid in vision tasks. Depth information
makes the determination of an object's underlying geometry easier,
leading to better identification~\cite{marton2010hierarchical}. Other
work has successfully incorporated RGB-D cameras into object
recognition in indoor domains featuring common household
objects~\cite{marton2010hierarchical,lai2011sparse}.

Our work combines both interaction with the world and expert feedback to
resolve uncertainty about the environment. Much of the previous work tends to focus
specifically on one of the two approaches.
Some researchers have used robotic manipulation in order to discover
information about an object like geometry, grasping points, and kinematics. Li
and Kleeman created a system that nudged objects in order to improve its
segmentation~\cite{li2008autonomous}. Katz and Brock used a robotic arm to
manipulate an object in order to obtain a kinematic
model~\cite{katz2008manipulating}.
These systems do not integrate interaction from
a user. Other systems do use human instruction to learn about the world, and
several of those use robots and simple toy objects to learn about colors,
shapes, and spatial relations. One system involved an environment very
similar to ours, but only used the robotic arm for pointing, not
manipulating~\cite{skocaj2007system}. Another involved teaching a
robot with foam blocks, but only used the robot to pick up an object
once it recognized it~\cite{zambuto2010visually}. The novelty of our
project comes in utilizing both exploration through manipulating
physical objects and active learning through human interaction. We
believe that both sources have different strengths and that combining
them lead to improved learning performance.

\section{Methodology}
In this section, we discuss the methodology behind the components of our
system, including vision, feature extraction, classification, learning, and
action.
%\xxx{How are you going to solve this problem? Why should the proposed method
%    work? Provide technical details of your approach if you are proposing a
%    novel method.}

\subsection{Point Cloud Acquisition}
Our work is intended to work with any colored 3D point cloud data. The
Kinect is a simple and cost effective platform for acquiring
this data. We interface with the Kinect using the OpenKinect
library~\cite{OpenKinect}. The Kinect broadcasts LCM~\cite{huang2010} messages
containing orientation and frame information for the system to process.

Aligning color and depth data from the Kinect requires not only
calibrating the individual color and IR cameras, but also cross calibrating
the two. To do this, we explored several existing toolkits, eventually
selecting RGBDemo~\cite{rgbdemo}. This kit automatically performs all of the
necessary calibrations, returning alignment parameters as well as terms for a
radial distortion model. Though the calibration returned has been sufficient
for our key tasks, it has not provided sufficient accuracy to enable more
complicated action such as gripping.

\subsection{Object Detection and Tracking}
%In order to play I Spy, the agent must be able to point at objects matching the
%provided description as well as to nudge objects with low confidence in their
%descriptions in order to see them from a new angle and gather new information
%about them. In order for the agent to do this, it is necessary to have localized
%positions for individual objects in the scene. Additionally, tracking objects
%between frames allows allows confidence levels to change over time.
Robust object tracking is necessary to know how the system's belief about an
object has changed after manipulation.
The RANSAC algorithm~\cite{fischler1981random} is used to discover the dominant
plane in each image. By removing this plane, we isolate the objects of
interest in the scene. Segmentation is performed to trying to discover
definitive object boundaries, considering both the change in
color and change in depth of neighboring pixels. These segments form colored
point cloud representations of each objects.
%Each pixel begins as its own segment, and the segments are iteratively
%combined with neighboring segments if the difference in color or the distance
%between them is less than a given threshold.
%From these segmentations we extract a feature vector for each object.

In order to adjust the confidence thresholds to determine which shapes are more
certain (discussed below), it is necessary to track an object over time. This can
be challenging as the physical object may change location over time and may be
occluded by the robot arm or another object. As each new frame appears, the
segmented objects in it are compared to segmented objects from prior frames.
We begin with the most recent frame and examine past frames if no matches were
found.  An object is considered a potential match to a prior object if the difference
in their mean color is small. If a potential match exists, a match is chosen
through a combination of physical and temporal proximity.

\subsection{Feature Extraction}
Once an object is segmented, we extract relevant features in order to classify
color, size, and shape. We limit our system to objects that were of a single color,
and use average RGB and HSV values across the pixels to construct a
six-dimensional color feature vector. Size is evaluated on two features: the
diagonal of the 3D, axis-aligned bounding box and the average distance of
points from the centroid.

Correctly classifying shapes requires more complicated features.
We explored several options for extracting features from
objects before settling on the method described below. We experimented
with ORB, a rotationally invariant feature extractor similar to SIFT and
SURF~\cite{rublee2011orb}. Additionally we tested several feature detectors
from OpenCV, including a square detector~\cite{opencv_library}. We also explored
using RANSAC to discover simple shape types (such as planes, curves, and spheres),
in order to determine the number of faces and edges each object has. This work
was based off the RANSAC algorithms of Schnabel et al~\cite{schnabel2007efficient}.
Although these and other feature detectors are frequently used in object recognition
and learning tasks, we found that their classifications generally not well
suited to describing our shapes. Thus, we ultimately chose to use a Principal Component Analysis
(PCA)-based method of our own devising.

To extract shape features, we apply PCA in order to find the axis with the
greatest variance in point distribution. We first project the points onto the
2D view plane. For $N$ pixels we compute the mean pixel location $\mu =
\frac{1}{N}\displaystyle\sum_{i=1}^{N}x_i$. Then we find the covariance matrix
$\Sigma = \frac{1}{N} BB^T$ where $B$ is an $2\times N$ matrix with the $i$th
column equal to $x_i - \mu$. The eigenvector with the largest eigenvalue
becomes the primary axis. We transform the image into a canonical representation
by finding a bounding box for the image aligned with the axes, then rotating and
scaling it to a unit box. To extract features we take evenly-spaced
points from the top and bottom edges of the box and calculate the vertical distance
from each point to the image. This process is illustrated by Fig.~\ref{fig:pca}. We use seven
sample points along the top and bottom; experimental evaluation showed no significant
improvement with additional samples. In addition, we calculate the ratio of the width
and height before scaling to estimate how elongated the shape is. For
training, we generate a total of four shape feature vectors from the supplied
image of the object by flipping the image vertically and horizontally. This allows
us to more quickly learn a shape across different orientations.

\begin{figure}[h!]
\centering
    \includegraphics[width=1.0\textwidth]{figures/PCA_example.png}
    \caption{This figure illustrates using PCA to extract shape features.
        First the principal component is found (A), then the image is rotated
        and scaled accordingly (B, C). Finally, points are sampled along
        the top and bottom and for each the vertical distance to the image
        is recorded (D).}
    \label{fig:pca}
\end{figure}

\subsection{Classification}
Initially we used SVM classification to validate our method, but we have since
moved to a K-Nearest Neighbor approach. With the KNN algorithm
it is easy to add another example to the training set without recreating the
entire training model as is done with SVM classification. Additionally, this
eliminates the need for a third-party SVM implementation. KNN is also well
suited for multi-class classification, removing
the need to do a one-vs-all (OVA) approach with multiple SVMs. Adding a new
class to the KNN classifier is also more straightforward and can be done dynamically. Using our
implementation of KNN classification, we are also able to easily construct and
report a confidence metric for each label.

The confidence of a label is determined by calculating the percentage of
neighbors with that label. The vision system experiences enough noise
to cause high variation in classification, particularly when the
confidence is low. To deal with this noise and to stabilize the reported labels,
our system keeps track of a moving window of the last 10 classifications; 15 for shape, which is
the most noisy. Based on experimental tuning, for color and shape we set $k=10$, while
for size we set $k=5$. The most popular historical label is reported along with its
confidence, which is calculated as a product of the frequency of that label
and the average confidence of all labels in the window.

\subsection{Confidence Thresholds and Object Manipulation}
One of the difficulties with reasoning about real objects in the world is
partial observability. Objects can have dramatically different appearances
at different orientations. Naturally, when given such an object, a human may move
his or her head around to gain more information and thus confidence as to what the object
is.  Similarly, when holding an unfamiliar object, a person may move it around
to further study it.

Our system has a fixed viewpoint, but can
manipulate objects to gain information on those whose appearance has high
orientation dependency. For this purpose, we adopt a confidence threshold system
to learn which objects have high uncertainty, and therefore are
be good candidates for manipulation. When the system is queried for an object
that it cannot currently see, it manipulates the uncertain object with the
highest probability of being the queried object.

All labels are given the same default confidence threshold, and then the
threshold adjusted based on user feedback.
We attempt to estimate $P(label_{actual}(obj) \neq x |
                                       label_{observed}(obj) = x)$,
which is the likelihood of a mislabel given the observed label $x$. For
example, the ``rectangle'' label is commonly misapplied to objects presented
head on. Thus our system learns that the confidence threshold for rectangle is
very high, meaning that $P(shape_{actual}(obj) \neq
                         rectangle|shape_{observed}(obj) = rectangle)$
is very high. When the system it asked to identify an object by a given
description it first tries to find an exact match and points to
it regardless of the confidence of the labels or the
corresponding confidence thresholds. As a result, when the agent
correctly identifies an object and the confidence is lower than the
threshold for the given label, the threshold for that label is lowered.
Intuitively, this is because the threshold was higher than needed for proper
identification.

If there are no objects that match exactly, the system examines partial
matches (i.e. matches with at least one correct label). For each incorrect label, it compares the
label confidence to that label's confidence threshold. If the confidence is
lower, it will attempt to nudge that object in the hopes that the label
is wrong. If multiple objects might be mislabeled it orders the search
based on the difference between the confidences and the thresholds. If, even after
manipulation, no objects are found matching the user description, the user is
prompted to identify the correct object. The thresholds for the incorrect
labels are then increased so that in the future, the system will be more
likely to nudge them.

\xxx{Justify min/max}
The thresholds are adjusted by a amount which decays with the number of
threshold updates. Additionally, thresholds are clamped between a minimum of
0.1 and a maximum of 0.95. Updates are calculated using the following formula:
$$\mathrm{Thresh_{t}} = \mathrm{Thresh_{t-1}} + \lambda^t C change$$
\[change = \left\{ \begin{array}{ll}
         1 & \mbox{if threshold should increase};\\
         -1 & \mbox{if threshold should decrease}.\end{array} \right. \]
where $t= \mathrm{number~of~threshold~updates}$ and C is a parameter. Through
experimentation, these values were tuned to $\lambda = 0.8$ and $C = 0.3$.

\subsection{Robot Arm Manipulation}
Our system is integrated with a custom robotic arm, which was loaned to us by Prof.
Edwin Olson. The arm geometry is simple, allowing the use of closed form inverse
kinematics solvers for various positions.

Because the Kinect is not statically located with respect to the robotic arm,
we perform an initial calibration so as to align the Kinect coordinate
frame with that of the arm. All coordinates are transformed such that the arm
is centered at the origin facing down the x-axis. We
may command the arm to ``point at'' or ``sweep'' an object, prompting the arm
to execute the appropriate scripted action at the object location. Pointing is
self-explanatory. Sweeping causes the arm to execute a nudging action,
swinging the arm through the object at an angle to encourage rotation.

\subsection{Active Learning}
Active learning is the process in which an agent continues to adjust its
understanding of its environment by querying the user to gain information.
Our system utilizes the knowledge of the users playing the game
to learn new labels and improve its confidence about known labels. The agent
requests information from users when it incorrectly identifies an
incorrect object or when it cannot find the specified object. The labels the
user provides in response to the agent are added as training examples into the
KNN algorithm, as are all correct guesses verified by the user.

The system is capable of beginning with no knowledge and, through playing games
with people and requesting feedback about whether its guesses are correct or
incorrect, learns to distinguish shapes, colors, and sizes. The
user may also choose to enter a ``training'' mode in which they provide
explicit examples of labels. This method allows user teach the system new labels.


\subsection{Data Collection and Training}
We acquired a variety of children's foam blocks in many shapes, sizes,
and colors to classify. A small subset of the
blocks can be seen in Figure~\ref{fig:blocks}. The foam of the blocks is an
ideal material to work with as it
\begin{inparaenum}[(1)]
\item has a smooth, matte finish,
\item comes in a variety of distinct colors, and
\item is deformable enough to be grabbed by the arm but still retain its
shape.
\end{inparaenum}
Additionally we collected household objects to test our classifier on,
demonstrating the ability of the system to classify properties some simple
real world objects.

Using our segmentation algorithm, we acquire point cloud representations of
our object, a small sample of which are shown in Fig.~\ref{fig:images}. From
these point clouds, we are able to extract features such as the HS features
pictured in Fig.~\ref{fig:colordata}.

\begin{figure}[h!]
\centering
    \includegraphics[width=.75\textwidth]{figures/screenshot.png}
    \caption{The main interface for the system. For each object the best labels and associated confidences are displayed.}
    \label{fig:screenshot}
\end{figure}
\section{Experimental Results}

Our system, seen in Fig.~\ref{fig:screenshot}, segments objects found in a
predefined play area and labels them with the most confident shape, size, and
color.  Additionally the user can train new shapes, colors, and sizes by simply
clicking on an object and supplying the relevant label. This method is not
efficient for generating a
large number of training samples of many different objects, so we collected a
training dataset of footage of the foam blocks at numerous positions and angles
which were segmented and hand labeled with relevant
descriptors. Our training data consists of 1857 shape examples, 1845 color
examples, and 1807 size examples.

\begin{figure}[h!]
\centering
\subfloat[Foam Blocks] {
    \includegraphics[height=1.6in]{figures/blocks.png}
    \label{fig:blocks}
}
\subfloat[Segmented Images] {
    \includegraphics[height=1.6in]{figures/blue_objs.png}
    \label{fig:images}
}
\subfloat[Real-World Test Data] {
    \includegraphics[height=1.6in]{figures/colorplot_noorange.eps}
    \label{fig:colordata}
}
\caption{Real world data is extracted from toy foam blocks. The blocks
    themselves can be seen in \subref{fig:blocks}. Using a segmentation
    algorithm, we extract the blocks from the Kinect data \subref{fig:images}
    and generate feature vectors for each object.
    Finally, we hand label the data for features such as color and shape for
    classification training. In \subref{fig:colordata}, we see the actual
    distribution of our training samples in the hue and saturation
    dimensions.}
\label{fig:objects}
\end{figure}

Testing our system requires direct interaction and the presentation of new objects
or views of objects at new angles. Additional testing can be done by training the
system with new labels and observing how quickly and effectively it
can learn to apply the new information. Qualitatively it is easy to see
if the system is performing well. To perform a less subjective, more quantitative
evaluation of our system, we used leave-one-out-cross-validation (LOOCV) to
evaluate our training data. This test mirrors the results of a real-world
scenario in which the user has already presented the system with nearly 2000 training
examples.

The results in Table~\ref{tbl:testresults} show LOOCV percentages
for the shape, color, and size. Unsurprisingly it has difficulty with shape,
which varies greatly based on orientation. Size, which is a more subjective
property of the object, also presents some difficulty. This can be largely
attributed to disagreement amongst the hand labels contributed by different
labelers. We observed that the system has particular problems with shapes that
are presented head on, which tend to resemble rectangles.  This was the primary
motivation for developing a confidence threshold learning mechanism, which
allows the system to determine which labels are inherently less informative.

\begin{table}
    \centering
    \begin{tabular}{||l|r||l|r||l|r||}
        \hline
        Shape         & Accuracy (\%) & Color   & Accuracy (\%) & Size &Accuracy (\%) \\ \hline \hline
        Triangle      & 95.07                & Red     & 100.00               & Small   & 87.97               \\ \hline
        T-shape       & 96.18                & Blue    & 100.00               & Medium  & 91.81               \\ \hline
        L-shape       & 96.48                & Green   & 98.88                & Large   & 81.70               \\ \hline
        Rectangle     & 90.54                & Yellow  & 100.00               & Overall & 86.11               \\ \hline
        Arch          & 93.86                & Orange  & 100.00               & ~       & ~                   \\ \hline
        Cylinder      & 90.94                & Purple  & 99.44                & ~       & ~                   \\ \hline
        Half-cylinder & 67.50                & Overall & 99.73                & ~       & ~                   \\ \hline
        Square        & 68.21                & ~       & ~                    & ~       & ~                   \\ \hline
        Overall       & 90.45                & ~       & ~                    & ~       & ~                   \\
        \hline
    \end{tabular}
    \caption{Results of LOOCV on nearly 2000 training examples for color, shape, and size.}
	\label{tbl:testresults}
\end{table}

\subsection{Example Scenario}

\section{Future Work}
\label{sec:future-work}
This work is a portion of a larger project, The Broad Operational Language
Translation Program (BOLT), which is a DARPA-funded research project spanning several
universities and industry research groups. The goal of BOLT is to create an
agent that learns to ground objects and actions in the real world through
interactive instruction. Our work focuses on the learning and object
classification components that the BOLT system will need.

The short-term goals of this continued work involve further expansion of the
action space to include precise manipulation and grasping of the objects. This
will allow the agent to position the objects for specific viewing angles as
well as place them in different positions in the play area. Additionally we will
interface our work with the Soar cognitive architecture. We had initially planned to
integrate Soar with this project, but found that it did not contribute
to the immediate goals of the project.

\section{Conclusion}
%\xxx{Summary of your progress and your final expected goal (what do you expect
%    to achieve or demonstrate for the final project?)}
We teach a system controlling a robotic arm to play the game I Spy.
In order to play I Spy, the system must be able to correctly identify labels
associated with objects in its environment. This requires identifying objects
and their salient features as well as using prior knowledge to map those features
to labels. We further extend the system to learn from mistakes in order to
improve future performance. When uncertain about the characteristics of an
object, the system may also nudge the object to change its perspective and
gain new information. Through playing, the system learns the inherent
uncertainties associated with particular labels. Our system provides the core
functionality of a complex agent capable of interacting with users and its
environment to ground language in physical attributes of objects in the world.

\bibliographystyle{abbrv}
\bibliography{literature}

\end{document}

% LocalWords:  multiclass
